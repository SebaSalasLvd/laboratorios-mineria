{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d8b815e",
   "metadata": {},
   "source": [
    "# Laboratorio 4: KNN\n",
    "\n",
    "Clase de Alexandre Gramfort, Anne Sabourin, y Joseph Salmon\n",
    "\n",
    "Actualizado por Valentin Barriere\n",
    "\n",
    "Integrantes: **José Pablo Canales y Sebastián Salas Lavado**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b994e26e",
   "metadata": {},
   "source": [
    "## Instrucciones\n",
    "\n",
    "1.  Trabajen en equipos de dos personas. Salvo excepciones, no se corregirá entregas con menos de dos integrantes.\n",
    "\n",
    "2.  Modifique este archivo `.ipynb` agregando sus respuestas donde corresponda.\n",
    "\n",
    "3.  Para cada pregunta **incluya el código fuente que utilizó para llegar a su respuesta. Respuestas sin código no recibirán puntaje.**.\n",
    "\n",
    "4.  El formato de entrega para esta actividad es un archivo **html**. **Genere un archivo HTML usando Jupyter** y súbalo a U-Cursos. Basta con que un/a integrante haga la entrega. Si ambos/as hacen una entrega en U-Cursos, se revisará cualquiera de éstas.\n",
    "\n",
    "\n",
    "#### **Se recomienda fuertemente que no usen ChatGPT para resolver la actividad, ya que la experiencia de aprendizaje es mucho mayor si lo hacen por su cuenta.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3204b3b",
   "metadata": {},
   "source": [
    "## Recordatorios  \n",
    "\n",
    "### Definiciones y Notación\n",
    "\n",
    "Recordamos aquí el marco de trabajo de la clasificación supervisada y presentamos las notaciones que se utilizarán a lo largo del laboratorio.\n",
    "\n",
    "- $\\mathcal{Y}$ es el conjunto de etiquetas de los datos. Aquí trabajamos con un número arbitrario $L$ de clases, y elegimos $\\mathcal{Y} = \\{1,\\dots, L\\}$ para representar las $L$ etiquetas posibles. El caso de clasificación binaria corresponde a $L=2$.  \n",
    "\n",
    "- $\\mathbf{x} = (x_1,\\dots,x_p)^\\top \\in \\mathcal{X} \\subset \\mathbb{R}^p$ es una observación, un ejemplo o un punto de muestra. La $j$-ésima coordenada de $\\mathbf{x}$ es el valor tomado por la $j$-ésima variable (característica).  \n",
    "\n",
    "- $\\mathcal{D}_n = \\{(\\mathbf{x}_i , y_i), i=1,\\dots, n\\}$ es el conjunto de entrenamiento que contiene los $n$ ejemplos y sus etiquetas.  \n",
    "\n",
    "- Existe un modelo probabilístico que gobierna la generación de nuestras observaciones a través de las variables aleatorias $X$ e $Y$: $\\forall i \\in \\{1,\\dots,n\\},  (\\mathbf{x}_i , y_i) \\stackrel{i.i.d.}{\\sim} (X,Y)$.  \n",
    "\n",
    "- Nuestro objetivo es construir, a partir del conjunto de entrenamiento $\\mathcal{D}_n$, una función llamada **clasificador**, $\\hat{f}:\\mathcal{X} \\rightarrow \\mathcal{Y}$, que asigne a un nuevo punto $\\mathbf{x}_{\\text{nuevo}}$ una etiqueta $\\hat{f}(\\mathbf{x}_{\\text{nuevo}})$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f236a24e",
   "metadata": {},
   "source": [
    "### Generación de Datos Artificiales  \n",
    "\n",
    "En esta sección, consideramos observaciones descritas en dos dimensiones (para facilitar la visualización), lo que significa que $p=2$ en el formalismo anterior. Utilizamos funciones que generan conjuntos de datos artificiales del script `tp_knn_source.py`, con modificaciones únicamente en los valores de las etiquetas.  \n",
    "\n",
    "1. **Estudia las funciones `rand_tri_gauss`, `rand_clown` y `rand_checkers`.**  \n",
    "   - ¿Qué devuelven estas funciones?  \n",
    "   - ¿A qué corresponde la última columna?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa330992",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tp_knn_source import (rand_gauss, rand_bi_gauss, rand_tri_gauss,\n",
    "                           rand_checkers, rand_clown, plot_2d,\n",
    "                           frontiere)\n",
    "\n",
    "############################################################################\n",
    "#     Generación de datos\n",
    "############################################################################\n",
    "\n",
    "n = 100\n",
    "mu = [1., 1.]\n",
    "sigma = [1., 1.]\n",
    "rand_gauss(n, mu, sigma)\n",
    "\n",
    "n1 = 20\n",
    "n2 = 20\n",
    "mu1 = [1., 1.]\n",
    "mu2 = [-1., -1.]\n",
    "sigma1 = [0.9, 0.9]\n",
    "sigma2 = [0.9, 0.9]\n",
    "data1 = rand_bi_gauss(n1, n2, mu1, mu2, sigma1, sigma2)\n",
    "\n",
    "n1 = 50\n",
    "n2 = 50\n",
    "n3 = 50\n",
    "mu1 = [1., 1.]\n",
    "mu2 = [-1., -1.]\n",
    "mu3 = [1., -1.]\n",
    "sigma1 = [0.9, 0.9]\n",
    "sigma2 = [0.9, 0.9]\n",
    "sigma3 = [0.9, 0.9]\n",
    "data2 = rand_tri_gauss(n1, n2, n3, mu1, mu2, mu3, sigma1, sigma2, sigma3)\n",
    "\n",
    "n1 = 50\n",
    "n2 = 50\n",
    "sigma1 = 1.\n",
    "sigma2 = 5.\n",
    "data3 = rand_clown(n1, n2, sigma1, sigma2)\n",
    "\n",
    "n1 = 150\n",
    "n2 = 150\n",
    "sigma = 0.1\n",
    "data4 = rand_checkers(n1, n2, sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61a3870",
   "metadata": {},
   "source": [
    "R:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1659bc03",
   "metadata": {},
   "source": [
    "2. Utiliza la función `plot_2d` para visualizar los conjuntos de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b20f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd047f9",
   "metadata": {},
   "source": [
    "## El Método de los $k$-Vecinos Más Cercanos  \n",
    "\n",
    "### Enfoque Intuitivo  \n",
    "\n",
    "El algoritmo de los $k$-vecinos más cercanos (**$k$-nn**) es un método intuitivo y fácil de ajustar para abordar problemas de clasificación con cualquier número de etiquetas.  \n",
    "\n",
    "El principio del algoritmo es simple: para cada nuevo punto $\\mathbf{x}$, primero determinamos el conjunto de sus $k$ vecinos más cercanos entre los puntos de entrenamiento, que denotamos como $V_k(\\mathbf{x})$ (por supuesto, debemos elegir $1 \\leq k \\leq n$ para que esto tenga sentido). La clase asignada al nuevo punto $\\mathbf{x}$ es entonces la **clase mayoritaria** dentro del conjunto $V_k(\\mathbf{x})$. En la siguiente figura se muestra una ilustración del método para el caso de tres clases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e72cee",
   "metadata": {},
   "source": [
    "### Ilustración\n",
    "\n",
    "![Visualización](knn.png \"Ejemplo de cómo funciona el método de los k-vecinos más cercanos para valores de parámetro k=5 y k=11. Consideramos tres clases, L=3, representadas respectivamente en negro (y=1), gris (y=2) y blanco (y=3)\")\n",
    "\n",
    "Ejemplo de cómo funciona el método de los k-vecinos más cercanos para valores de parámetro k=5 y k=11. Consideramos tres clases, L=3, representadas respectivamente en negro (y=1), gris (y=2) y blanco (y=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8219b0d",
   "metadata": {},
   "source": [
    "1. Proponga una versión adaptada de este método para **regresión**, *es decir*, cuando las observaciones tienen valores reales: $\\mathcal{Y} = \\mathbb{R}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c923b0e",
   "metadata": {},
   "source": [
    "R: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ac30e2",
   "metadata": {},
   "source": [
    "## Enfoque Formal\n",
    "\n",
    "Para definir con precisión el método, primero debemos elegir una distancia $d: \\mathbb{R}^p \\times \\mathbb{R}^p \\rightarrow \\mathbb{R}$. Para un nuevo punto $\\mathbf{x}$, definimos entonces el conjunto de sus $k$-vecinos más cercanos $V_k(\\mathbf{x})$ usando esta distancia. Procedemos de la siguiente manera:\n",
    "\n",
    "Para cada $\\mathbf{x} \\in \\mathbb{R}^p$ y cada $i = 1, \\ldots, n$, denotamos $d_i(\\mathbf{x})$ como la distancia entre $\\mathbf{x}$ y $\\mathbf{x}_i$:\n",
    "$$\n",
    "d_i(\\mathbf{x}) = d(\\mathbf{x}_i, \\mathbf{x}).\n",
    "$$\n",
    "\n",
    "Definimos el estadístico de primer orden $r_1(\\mathbf{x})$ como el índice del vecino más cercano a $\\mathbf{x}$ entre $\\mathbf{x}_1, \\ldots, \\mathbf{x}_n$, es decir:\n",
    "$$\n",
    "r_1(\\mathbf{x}) = i^* \\quad \\text{si y solo si} \\quad d_{i^*}(\\mathbf{x}) = \\min_{1 \\le i \\le n} d_i(\\mathbf{x}).\n",
    "$$\n",
    "\n",
    "> **Nota**  \n",
    "> Si hay múltiples candidatos para el mínimo anterior, los empates se resuelven arbitrariamente (generalmente al azar).\n",
    "\n",
    "Por inducción, podemos definir el rango $r_k(\\mathbf{x})$ para cualquier entero $1 \\leq k \\leq n$:\n",
    "$$\n",
    "r_k(\\mathbf{x}) = i^* \\quad \\text{si y solo si} \\quad\n",
    "d_{i^*}(\\mathbf{x}) = \\min_{\\substack{1 \\le i \\le n \\\\ i \\notin \\{r_1, \\ldots, r_{k-1}\\}}} d_i(\\mathbf{x}).\n",
    "$$\n",
    "\n",
    "El conjunto de los $k$-vecinos más cercanos de $\\mathbf{x}$ se escribe entonces:\n",
    "$$\n",
    "V_k(\\mathbf{x}) = \\{ \\mathbf{x}_{r_1}, \\dots, \\mathbf{x}_{r_k} \\}.\n",
    "$$\n",
    "\n",
    "Finalmente, la decisión para clasificar el punto $\\mathbf{x}$ se toma por votación mayoritaria, resolviendo el siguiente problema:\n",
    "\n",
    "$$\n",
    "\\hat{f}_k(\\mathbf{x}) \\in \\arg\\max_{y \\in \\mathcal{Y}} \\left( \\sum_{j=1}^k \\mathbb{1}_{\\{Y_{r_j} = y\\}} \\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a53c90b",
   "metadata": {},
   "source": [
    "El módulo `sklearn.neighbors` de **scikit-learn** implementa métodos de clasificación y regresión basados en los $k$-vecinos más cercanos.  \n",
    "Consulta la documentación: [scikit-learn.neighbors](http://scikit-learn.org/stable/modules/neighbors.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4cc7e2",
   "metadata": {},
   "source": [
    "2. Completa la clase `KNNClassifier`.  \n",
    "   Verifica la exactitud de los resultados comparando con la clase `KNeighborsClassifier` de **scikit-learn**.  \n",
    "   Propón **tu propio método de comparación**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e96d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "\n",
    "class KNNClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, n_neighbors=1):\n",
    "        self.n_neighbors = n_neighbors\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.X_ = X\n",
    "        self.y_ = y\n",
    "        return self\n",
    "\n",
    "    def predict(X):\n",
    "        n_samples, n_features = X.shape\n",
    "        # TODO : Calcular todas las distancias por pares entre X y self.X_\n",
    "\n",
    "        # TODO : Encuentre las etiquetas predichas Y para cada entrada X\n",
    "        # Puede utilizar la función scipy.stats.mode\n",
    "\n",
    "        return np.zeros(n_samples)\n",
    "\n",
    "# TODO : Compare su implementación con scikit-learn\n",
    "from sklearn import neighbors\n",
    "\n",
    "# Conjunto de datos 2\n",
    "X_train = data2[::2, :2]\n",
    "Y_train = data2[::2, 2]\n",
    "X_test = data2[1::2, :2]\n",
    "Y_test = data2[1::2, 2]\n",
    "\n",
    "# ...\n",
    "\n",
    "\n",
    "# De ahora en adelante utilice la implementación de scikit-learn\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd730f5",
   "metadata": {},
   "source": [
    "**Nota**: Para ahorrar en tiempo de cómputo, deberás utilizar la implementación de **scikit-learn**!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad24c9f",
   "metadata": {},
   "source": [
    "3. Ejecuta el algoritmo de clasificación en los tres conjuntos de datos de ejemplo,  \n",
    "   utilizando la **distancia euclidiana clásica**:  \n",
    "   $$\n",
    "   d(\\mathbf{x}, \\mathbf{v}) = \\| \\mathbf{x} - \\mathbf{v} \\|_2\n",
    "   $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97859544",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors = 5  # la k en k-NN\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "\n",
    "plt.figure(figsize=(20, 5))\n",
    "for i, data in enumerate([data1, data2, data3, data4]):\n",
    "    # TODO : Entrenar el clasificador KNN con los datos de entrenamiento\n",
    "    # TODO : Generar un gráfico con los puntos y la frontera de decisión del clasificador\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3dc994e",
   "metadata": {},
   "source": [
    "4. Varía el número $k$ de vecinos considerados.  \n",
    "   - ¿En qué se convierte el método en los casos extremos cuando $k=1$? ¿Y cuando $k=n$?  \n",
    "   - Muestra estos casos en los datos estudiados.  \n",
    "   - ¿En qué casos el **límite de decisión es complejo**? ¿**simple**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0539199",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(3, figsize=(12, 8))\n",
    "for n_neighbors in range(1, 16):\n",
    "    knn = neighbors.KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "\n",
    "    # TODO : Entrene el knn\n",
    "    \n",
    "    plt.subplot(3, 5, n_neighbors)\n",
    "    plt.xlabel('KNN with k=%d' % n_neighbors)\n",
    "    plot_2d(X_train, Y_train)\n",
    "    frontiere(knn.predict, X_train, step=50, tiny=True)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be1fdab",
   "metadata": {},
   "source": [
    "R:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911819f5",
   "metadata": {},
   "source": [
    "5. Una variante ampliamente utilizada consiste en **ponderar los votos** del j-ésimo vecino mediante la siguiente fórmula:  \n",
    "   $$\n",
    "   e^{-d_j^2 / h}\n",
    "   $$\n",
    "   (donde $h$ controla el nivel de ponderación).\n",
    "\n",
    "   Esto equivale a reemplazar la ecuación de votación mayoritaria por:\n",
    "\n",
    "   $$\n",
    "   \\hat{f}_k(\\mathbf{x}) \\in \\arg\\max_{y \\in \\mathcal{Y}} \\left( \\sum_{j=1}^k \\exp\\left(-\\frac{d_j^2}{h}\\right) \\mathbb{I}_{\\{Y_{r_j} = y\\}} \\right)\n",
    "   $$\n",
    "\n",
    "   - Implementa esta variante en tu clase `KNNClassifier`\n",
    "   - También impleméntala usando scikit-learn pasando el parámetro `weights` al constructor de `KNeighborsClassifier`\n",
    "   - Puedes inspirarte en la función `_weight_func` de las pruebas de scikit-learn: [Enlace a prueba de scikit-learn](https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/neighbors/tests/test_neighbors.py)\n",
    "\n",
    "   - **Prueba el impacto de la elección de $h$** en los límites de clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a1bb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights(dist):\n",
    "    \"\"\"\n",
    "    Devuelve un array de pesos, que disminuyen exponencialmente al cuadrado\n",
    "    de la distancia.\n",
    "\n",
    "    Parámetros\n",
    "    ----------\n",
    "    dist : un array unidimensional de distancias.\n",
    "\n",
    "    Retorna\n",
    "    -------\n",
    "    weight : array del mismo tamaño que dist\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    return None\n",
    "\n",
    "n_neighbors = 5\n",
    "wknn = neighbors.KNeighborsClassifier(n_neighbors=n_neighbors, weights=weights)\n",
    "wknn.fit(X_train, Y_train)\n",
    "plt.figure(4)\n",
    "plot_2d(X_train, Y_train)\n",
    "frontiere(wknn.predict, X_train, step=50)\n",
    "\n",
    "print(wknn.predict(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd43570",
   "metadata": {},
   "source": [
    "Comente sus resultados\n",
    "\n",
    "R:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3506bb",
   "metadata": {},
   "source": [
    "6. ¿Cuál es la **tasa de error** en tus datos de entrenamiento (es decir, la proporción de errores cometidos por el clasificador) cuando $k = 1$?\n",
    "\n",
    "    Use el método [`score()`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier.score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b02c294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af40c835",
   "metadata": {},
   "source": [
    "7. Grafica las **curvas de error** en función de $k$ para uno de los conjuntos de datos,  \n",
    "   con tamaños de muestra $n$ que varíen entre $100$, $500$ y $1000$.\n",
    "\n",
    "   - ¿Cuál es el mejor valor de $k$?\n",
    "   - ¿Es siempre el mismo para los diferentes conjuntos de datos?\n",
    "   - Asegúrate de evaluar el error en los **datos de prueba**.\n",
    "   - Puedes utilizar la clase proporcionada `ErrorCurve`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9981b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar\n",
    "sigma = 0.1\n",
    "range_n_samples = [100, 500, 1000]\n",
    "k_range = range(1, 100)\n",
    "\n",
    "# Función auxiliar para generar los conjuntos de entrenamiento y prueba\n",
    "def generate_train_test(n, sigma):\n",
    "    data = rand_checkers(n, n, sigma)\n",
    "    X_train, Y_train = data[:, :2], data[:, 2]\n",
    "    data_test = rand_checkers(n, n, sigma)\n",
    "    X_test, Y_test = data_test[:, :2], data_test[:, 2]\n",
    "    return X_train, Y_train, X_test, Y_test\n",
    "\n",
    "from tp_knn_source import ErrorCurve\n",
    "\n",
    "# TODO : Graficar la curva de error para cada valor de k y n_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17691a1",
   "metadata": {},
   "source": [
    "R:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e4d902",
   "metadata": {},
   "source": [
    "8. En tu opinión, ¿cuáles son las **ventajas** y **desventajas** del método de los vecinos más cercanos?  \n",
    "   - ¿Tiempo de cálculo?  \n",
    "   - ¿Escalabilidad?  \n",
    "   - ¿Interpretabilidad?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416897d3",
   "metadata": {},
   "source": [
    "R:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559c4d15",
   "metadata": {},
   "source": [
    "9. Aplica el método al conjunto de datos **`digits`** con diferentes elecciones de $k \\geq 1$. \n",
    "\n",
    "    Consulta: [plot_digits_classification.py — scikit-learn](https://scikit-learn.org/stable/auto_examples/classification/plot_digits_classification.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ed6422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sklearn\n",
    "from sklearn import datasets\n",
    "\n",
    "# El conjunto de datos de dígitos\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "print(type(digits))\n",
    "# Un Bunch es una subclase de 'dict' (dictionary)\n",
    "# help(dict)\n",
    "# vea también \"http://docs.python.org/2/library/stdtypes.html#mapping-types-dict\"\n",
    "\n",
    "plt.close(7)\n",
    "plt.figure(7)\n",
    "for index, (img, label) in enumerate(list(zip(digits.images, digits.target))[10:20]):\n",
    "    plt.subplot(2, 5, index + 1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(img, cmap=plt.cm.gray_r, interpolation='None')\n",
    "    plt.title('Training: %i' % label)\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(digits.target)\n",
    "\n",
    "n_samples = len(digits.data)\n",
    "\n",
    "X_train = digits.data[:n_samples // 2]\n",
    "Y_train = digits.target[:n_samples // 2]\n",
    "X_test = digits.data[n_samples // 2:]\n",
    "Y_test = digits.target[n_samples // 2:]\n",
    "\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors=30)\n",
    "knn.fit(X_train, Y_train)\n",
    "\n",
    "score = knn.score(X_test, Y_test)\n",
    "print('Score : %s' % score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a3061a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : Probar con distintos valores de k y comparar el rendimiento\n",
    "# Pueden usar ErrorCurve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93392b24",
   "metadata": {},
   "source": [
    "10. Grafica la **matriz de confusión** $\\left( \\mathbb{P}\\{Y = i, C_k(X) = j\\} \\right)_{i, j}$ asociada al clasificador $C_k$. \n",
    "\n",
    "    Para manejar estas matrices con scikit-learn, consulta: [Ejemplo de matriz de confusión — scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e78d54ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "Y_pred = knn.predict(X_test)\n",
    "\n",
    "# TODO : Graficar la matriz de confusión"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51248763",
   "metadata": {},
   "source": [
    "11. Propón un **método para seleccionar $k$** e impleméntalo. Puedes utilizar la clase proporcionada `LOOCurve`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cc21e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revise la clase 'LOOCurve', en el archivo fuente\n",
    "\n",
    "from tp_knn_source import LOOCurve\n",
    "\n",
    "loo_curve = LOOCurve(k_range=list(range(1, 50, 5)) + list(range(100, 300, 100)))\n",
    "\n",
    "# TODO : Graficar LOOCurve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbe3e62",
   "metadata": {},
   "source": [
    "¿De qué forma elegiría usando `LOOCurve` k?\n",
    "\n",
    "R:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
